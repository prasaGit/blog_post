{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nPicking the right machine learning algorithm is critical because it decide the performance of the model. \n\nThe model can be chosen based on performance often calculated by the `KFold-cross-validation` technique.\n\nThe high mean performance is better than the lower mean performance. But what if it caused by just a statistical luck? \n\nTo address this issue, we can apply the **statistical hypothesis testing** to evaluate the difference in the mean performance resulting from the cross validation. If the difference is above the significance level `p-value` we can reject the null hypothesis that the two algorithms are the same. Hence we can pick the best ML model.  "},{"metadata":{},"cell_type":"markdown","source":"# Tutorial Objectives\n\n1. Model selection based on mean performance score could be misleading.\n2. Why using the Paired Strudent's t-test over the original Student's t-test.\n3. Applying the advance technique of 5X2 fold by utilizing the `MLxtend` library for comparing the algorithms based on `p-value` \n"},{"metadata":{},"cell_type":"markdown","source":"# Table of content\n\n1. Extract the best two models\n2. Steps to conduct hypothesis testing on the best two\n3. Steps to apply the 5X2 fold\n4. Comparing Classifier algorithms"},{"metadata":{},"cell_type":"markdown","source":"# Loading dataset"},{"metadata":{},"cell_type":"markdown","source":"For this tutorials, I will make use of the `load_iris` dataset within the `sklearn` library. However, the steps are the same for any ML problem. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_iris\n# loading iris dataset as datafram\ndf = load_iris(as_frame = True)","execution_count":73,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.feature_names","execution_count":74,"outputs":[{"output_type":"execute_result","execution_count":74,"data":{"text/plain":"['sepal length (cm)',\n 'sepal width (cm)',\n 'petal length (cm)',\n 'petal width (cm)']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = df.data\ndataset.head()","execution_count":75,"outputs":[{"output_type":"execute_result","execution_count":75,"data":{"text/plain":"   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0                5.1               3.5                1.4               0.2\n1                4.9               3.0                1.4               0.2\n2                4.7               3.2                1.3               0.2\n3                4.6               3.1                1.5               0.2\n4                5.0               3.6                1.4               0.2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the target variable\ny = df.target\ny.shape","execution_count":85,"outputs":[{"output_type":"execute_result","execution_count":85,"data":{"text/plain":"(150,)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the independent variables\nX = df.data\nX.shape","execution_count":86,"outputs":[{"output_type":"execute_result","execution_count":86,"data":{"text/plain":"(150, 4)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count how many classes per target - make sure it is a balance data\nprint(dataset.groupby(df.target).size())","execution_count":77,"outputs":[{"output_type":"stream","text":"target\n0    50\n1    50\n2    50\ndtype: int64\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Steps to extract the best two models\n\nIn this steps, I will conduct a comparasion between four different algorithms based on performance accuracy score. Then will select the two models with the highest score to conduct hypothesis testing between them.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import mean, std\nfrom sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n","execution_count":78,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, y_train, X_test, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)","execution_count":101,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Spot-Check Algorithms\nmodels = []\nmodels.append(('LR', LogisticRegression(max_iter=1000))) \nmodels.append(('LDA', LinearDiscriminantAnalysis())) \nmodels.append(('KNN', KNeighborsClassifier())) \nmodels.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = RepeatedStratifiedKFold(n_splits=10, n_repeats = 3, random_state=1)\n    cv_results = cross_val_score(model, X, y, cv=kfold, scoring='accuracy') \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n","execution_count":110,"outputs":[{"output_type":"stream","text":"LR: 0.964444 (0.041216)\nLDA: 0.980000 (0.030551)\nKNN: 0.964444 (0.037450)\nSVM: 0.964444 (0.044666)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"It seems that `LR` and `KNN` and `SVM` has same mean. However, `LDA` shows a higher mean accuracy over the rest of algorithms. Let's develop a boxplot betweeen `KNN` and `LDA` as a visualization for more interpretation. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n# evaluate model 1\nmodel1 = LinearDiscriminantAnalysis()\ncv1 = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)\nscores1 = cross_val_score(model1, X, y, scoring = 'accuracy', cv = cv1, n_jobs = -1)\nprint('LDA Mean Accuracy: %.3f (%.3f)' % (mean(scores1), std(scores1)))\n\n# evaluate model 2\nmodel2 = KNeighborsClassifier()\ncv2 = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)\nscores2 = cross_val_score(model2, X, y, scoring = 'accuracy', cv = cv2, n_jobs = -1)\nprint('KNeighborsClassifier Mean Accuracy: %.3f (%.3f)' % (mean(scores2), std(scores2)))\n\n# plot the results\nplt.boxplot([scores1, scores2], labels=['LDA', 'KNN'], showmeans=True)\nplt.show()","execution_count":114,"outputs":[{"output_type":"stream","text":"LDA Mean Accuracy: 0.980 (0.031)\nKNeighborsClassifier Mean Accuracy: 0.964 (0.037)\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"Seems that LDA has a better performance over KNN where LDA has a higher accuracy.\n\nLet's conduct a hypothesis test on those two algorithms to pick the best one.\n"},{"metadata":{},"cell_type":"markdown","source":"# The intuition behind the 5X2 fold approach\n\nOne approach is to evaluate each model on the same k-fold cross-validation split of the data (e.g. using the same random number seed to split the data in each case) and calculate a score for each split. This would give a sample of 10 scores for 10-fold cross-validation. The scores can then be compared using a paired statistical hypothesis test because the same treatment (rows of data) was used for each algorithm to come up with each score. The Paired Student’s t-Test could be used.\n\nA problem with using the Paired Student’s t-Test, in this case, is that each evaluation of the model is not independent. This is because the same rows of data are used to train the data multiple times — actually, each time, except for the time a row of data is used in the hold-out test fold. This lack of independence in the evaluation means that the Paired Student’s t-Test is optimistically biased.\n\nThis statistical test can be adjusted to take the lack of independence into account. Additionally, the number of folds and repeats of the procedure can be configured to achieve a good sampling of model performance that generalizes well to a wide range of problems and algorithms. Specifically two-fold cross-validation with five repeats, so-called 5×2-fold cross-validation.\n\nThis approach was proposed by Thomas Dietterich in his 1998 paper titled “Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms.”"},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.evaluate import paired_ttest_5x2cv\n# check if difference between algorithms is real\nt, p = paired_ttest_5x2cv(estimator1=model1, \n                          estimator2=model2, \n                          X=X, \n                          y=y, \n                          scoring='accuracy', \n                          random_seed=1)\n# summarize\nprint('P-value: %.3f, t-Statistic: %.3f' % (p, t))\n# interpret the result\nif p <= 0.05:\n    print('Difference between mean performance is probably real')\nelse:\n    print('Algorithms probably have the same performance')","execution_count":115,"outputs":[{"output_type":"stream","text":"P-value: 0.229, t-Statistic: 1.369\nAlgorithms probably have the same performance\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}